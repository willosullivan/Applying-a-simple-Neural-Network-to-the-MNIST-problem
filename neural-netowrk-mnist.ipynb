{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Neural Network to solve the MNIST problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we’re trying to solve here is to classify grayscale images of handwritten digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the MNIST\n",
    "dataset, a classic in the machine-learning community, which has been around almost\n",
    "as long as the field itself and has been intensively studied. It’s a set of 60,000 training\n",
    "images, plus 10,000 test images, assembled by the National Institute of Standards and\n",
    "Technology (the NIST in MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Solving\" MNIST can be thought of as the \"Hello World\" of Deep Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we jump in:\n",
    "\n",
    "    Understanding the OOP paradigm\n",
    "\n",
    "Object Orientated Programming is a programming paradigm that came after procedural/scripting and funcitonal. OOP turns logic into objects that can be easily reused in quick and reader friednly ways. \n",
    "\n",
    "It is important to be familiar with principles of OOP if you want to use ML for Python as the models you use (e.g. Logistic Regression, KNN, Neural Networks) will be **Classes** and their associated behaviours (\".fit\" to train the model on training data, \".add\" to add your model parameters as kwargs and args) will be **Methods** of that class. \n",
    "\n",
    "A very TLDR, use the link below or Google to find out more:\n",
    "- Objects are instances of a Class\n",
    "- Classes are a template for some Object you are programming (a Neural Network, in our case)\n",
    "- If Classes are the template for some object, consider methods the possible behaviours of that Class\n",
    "    \n",
    "https://www.youtube.com/watch?v=B5O52_w-4Zg&feature=emb_logo\n",
    "\n",
    "\n",
    "    TLDR for Neural Networks\n",
    "\n",
    "A neural network is a learning object that consits of layers of tensors which all take in and output data. Inbetween the input and output, the tensor in question makes a simple linear transformation to the data structure (a Numpy array, explain below). It can help to think of *layer* as a data preprocessing module, which acts to filter the data that comes through it and to output it in a slighly more useful form. You use kwargs and args to tune the model, training it to perform equally as well on testing and training data. All the while minimising overfitting, the cornerstone problem in machine learning.\n",
    "\n",
    "    Purpose of a Neural Network\n",
    "\n",
    "Chain together simple linear transformations on batches of random samples of data, desgined to optimise the model towards the metric representing success to the probem at hand.\n",
    "\n",
    "    \n",
    "    How we convert our data into machine learning friendly currency\n",
    "\n",
    "We encode our data as Numpy arrays. **Numpy arrays are matricies**. Here is how we encode different types of data:\n",
    "\n",
    " - *Vector data:* 2D tensor of shape (samples, features)\n",
    " - *Timeseries* or *sequence data:* 3D tensor of shape (samples, timesteps, features)\n",
    " - *Images:* 4D tensor of shape (samples, height, width, channels)\n",
    " - *Video:* 5D tensor of shape (sample, frames, height, width, channels) \n",
    "\n",
    "This is way funner than Year 12..\n",
    "\n",
    "\n",
    "\n",
    "###     \n",
    "    \n",
    "    Anatomy of a Neural Network\n",
    "    \n",
    "- Layers (which form the network)\n",
    "- Input data (samples) and associated targets (labels)\n",
    "- The loss function, which defines the feedback signal uesd for learning\n",
    "- The optimiser, which orchestrates how the learning proceeds\n",
    "- The designated metric to optimise towards, often by reducing errors (e.g. MAE, MSE)\n",
    "\n",
    "Together, these components form the model. I examine each area in more depth in my notes.\n",
    "    \n",
    "    Compiling a Neural Network\n",
    "\n",
    "- A loss function (https://keras.io/api/losses/): how the network identifies errors (e.g. MSE, MAE) during training and steers itself in the right direction\n",
    "\n",
    "\n",
    "- An optimiser (https://keras.io/api/optimizers/): the mechaism through which the network updates itself based on the data it sees and its loss function\n",
    "\n",
    "\n",
    "- The metric to monitor success during training and testing (https://keras.io/api/metrics/): for this demonstration I will only look at accuracy; the fraction of images correctly classified\n",
    "\n",
    "I think of it as one big feedback loop. I examine each area in more depth in my notes.\n",
    "   \n",
    "    \n",
    "    A ML Workflow\n",
    "\n",
    "\n",
    "\n",
    "    The mathematical mechanisms behind a Neural Network learning\n",
    "**The gears of neural networks: tensor operations**\n",
    "\n",
    "- Element-wise operations\n",
    "- Broadcasting\n",
    "- Tensor dot\n",
    "- Tensor reshaping\n",
    "- Geometric interpretation of tensor operations\n",
    "- Geometric interpretation of deep learning\n",
    "\n",
    "**The engine of neural networks: gradient-based optimisation**\n",
    " \n",
    "- What is a derivative?\n",
    "- Derivaties of tensor operations: the gradient\n",
    "- Stochatic gradient descent\n",
    "- Chaining derivatives: the Backpropagation algorithm\n",
    "\n",
    "The nit and grit that powers deep learning. I examine each area in more depth in my notes and profess to be challenged by the content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jumping in: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset in Keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*train_images* and *train_labels* for the training set which is the data the model will learn from. We will then train the model on the test data: *test_images* and *test_labels*.\n",
    "\n",
    "The images are encoded into Numpy arrays and the labels are an array of digits, from 0-9. << For more information, refer to attached Notion notes. \n",
    "\n",
    "TLDR: Data representaitons for neural networks.\n",
    "\n",
    "\n",
    "Images and labels have a one-to-one correspondence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network architecture\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The compilation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The compilation step\n",
    "\n",
    "network.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the image data (data \"pre-processing\")\n",
    "\n",
    "Before we train the network, we need to preprocess the data by reshaping it into the shape the network is expecting. We then need to scale the data so all values are in the [0, 1] interval. We know that previously our training images were stored in an array with dimensions (60000, 28, 28), as there were 60000 images of 28 x 28 pixels of type *unit8* with values in the [0, 255] interval. \n",
    "\n",
    "Below we tranform our data into a *float32* array of shape (60000, 28 * 28) with values between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the image data\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the labels (also data \"pre-processing\")\n",
    "\n",
    "We also need to categorically encode the labels, a step that’s explained in chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the labels\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing the Neural Network!!\n",
    "\n",
    "We train our model by calling the network's *fit method*. As with all macine learning I have encountered (scikitlearn and now Keras), it's important to understand Object Orientated Programming as we are using Objects (Classes and Methods) when we do ML with these libraries. \n",
    " \n",
    "    Objects are instances of a Class\n",
    "    Classes are a template for some Object you are programming (a Neural Network, in our case)\n",
    "    If Classes are the template for some object, consider methods the possible behaviours of that Class\n",
    "     \n",
    "    When we train our model by calling the \"fit\" method, we are calling the train behaviour of the tf.keras.Model class. Here we have the tf.keras.Model class stored as the variable \"network\". \n",
    "\n",
    "Give Google a crack if you want to learn more about OOP.\n",
    "\n",
    "Read here for more about the Keras Model class: https://www.tensorflow.org/api_docs/python/tf/keras/Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2587 - accuracy: 0.9253\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1044 - accuracy: 0.9684\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0681 - accuracy: 0.9794\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0492 - accuracy: 0.9853\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0369 - accuracy: 0.9891\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0287 - accuracy: 0.9916\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0214 - accuracy: 0.9940\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0171 - accuracy: 0.9951\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0131 - accuracy: 0.9962\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0099 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2493ca51b80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the network\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some terminology**: \n",
    "\n",
    "An *epoch* is a hyperparameter (argument) that describes the numer of times the neural network will see the entire datset. I have ours set to 10, so the neural network will run through the datset 10 times and thus have 10 opportunities to adjust the layer weighting (parametres) and -->> **learn** <<--\n",
    "\n",
    "The last epoch achieved an accuracy of 0.997, meaning it classified the train images correctly over 99% of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0662 - accuracy: 0.9833\n",
      "test_acc: 98.33 %\n"
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    " \n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', round(test_acc*100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test-set accuracy comes out at 98.32%, meaning our network/model correctly classified the test images to the labels 0-9 98.13% of the time. However, there is a significant gap between our training and testing accuracy.\n",
    "\n",
    "The above phenomenon is an example of *overfitting*, whcih describes the scenario where which a machine learning model tends to perform worse on new data (test data) compared to the data it trained on. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61960080091651cf711893763c0fc63533c6bdb38c1f3ab92d8591aae5c070c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
